# ECE1512 â€“ Project B Knowledge Distillation for Building Lightweight Deep Learning Models in Visual Classification Tasks

This project aims to implement knowledge distillation on two datasets, MNIST and MHIST. When training the student model using the teacher model, we need to consider the effect of hyperparameters alpha and temperature on knowledge distillation. A better model can be obtained by tune these two hyperparameters. Also, we need to compare the difference between student model with KD and student model without KD to understand the role of KD. We also use XAI methods to help understand the performance of the model. Based on the original KD, we also learn the further KD approach of TAKD and compare the student model obtained by KD and TAKD.

Task 1: KNOWLEDGE DISTILLATION IN MNIST DATASET

TASK 2: KNOWLEDGE DISTILLATION IN MHIST DATASET

We used RISE as our XAI Methods to explane all the models and we use TAKD as our state-of-the-arts KD algorithm.
